import argparse
import calendar
import pyspark
from pyspark.sql import SparkSession
from typing import Tuple
import datetime
from datetime import date, timedelta
from datetime import timedelta
from pyspark.sql import functions as F
from pyspark.sql.functions import *
from pyspark.sql import DataFrame
PySparkDF = pyspark.sql.dataframe.DataFrame
from pyspark.sql.window import Window
from pyspark.sql import SparkSession
from pyspark.sql.types import *
import numpy as np
import sys
import json
import time
from functools import reduce
from pyspark.sql.functions import pandas_udf, PandasUDFType
import pandas as pd
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType



def get_bq_spark_session(project_id, bq_dataset, ds_temp_bucket):
    return (
        SparkSession.builder.config("viewsEnabled", "true")
        .config("materializationProject", project_id)
        .config("materializationDataset", bq_dataset)
        .config("temporaryGcsBucket", ds_temp_bucket)
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")
        .getOrCreate()
    )
    
def read_bq_table_to_df(spark: SparkSession, sql_query: str) -> PySparkDF:
    return spark.read \
        .format('bigquery') \
        .option('query', sql_query) \
        .load()
        
     

def write_df_to_bq_table(df: PySparkDF, write_mode: str, bq_table: str) -> None:
    df.write \
        .mode(write_mode) \
        .format('bigquery') \
        .option('intermediateFormat', 'orc') \
        .option('table', bq_table) \
        .save()

def get_date_range() -> Tuple[str, str]:
    begin_iso = CUT_OFF_DATE_START
    end_iso = CUT_OFF_DATE_END
    return (begin_iso, end_iso)


def merge_map_inv_scan_table():
    sql_mp_inv_scan=f"""
    select 
    item_mapping.parent_item_nbr,
    item_mapping.child_item_nbr,
    item_mapping.club_nbr,
    coalesce(inv.inventory_qty,0) as inventory_qty,
    coalesce(scan.unit_qty_1w, 0) as unit_qty_1w
    from
    (select distinct 
            df.item_nbr as parent_item_nbr, 
            xref.item_nbr as child_item_nbr,  
            df.club_nbr, 
            df.date,
            df.unit_sold_cnt
    from {SECOND_LEVEL_FEATURES} as df
    inner join
    {MAPPING_TABLE} as xref
    on df.item_nbr = xref.parent_item_nbr
    where df.date = (SELECT MAX(DATE) - INTERVAL 1 DAY FROM  {SECOND_LEVEL_FEATURES})) as item_mapping
    left join
    (select item_nbr, club_nbr, snapshot_date, max(inventory_qty) as inventory_qty from
    (select *, 
    CASE 
        WHEN dept_nbr in (42,44,57) THEN onsite_onhand_qty+on_order_qty
        ELSE onsite_onhand_qty
        END AS inventory_qty  
    from 
    
            ( SELECT 
                INV.system_item_nbr,
                INV.snapshot_date,
                INV.onsite_onhand_qty,
                INV.on_order_qty,
                INV.item_on_shelf_date_dt,
                INV.club_nbr,
                ID.item_nbr,
                ID.dept_nbr,
                ID.subclass_nbr
            FROM
            (SELECT 
                cast(item_nbr as INT) as system_item_nbr,
                snapshot_date,
                onsite_onhand_qty,
                on_order_qty,
                item_on_shelf_date as item_on_shelf_date_dt,
                club_nbr
            FROM {MDSE_INV_DLY}
            WHERE SNAPSHOT_DATE >= "{begin_iso}"
                and SNAPSHOT_DATE <="{end_iso}"
                and STATUS not in ('R', 'L', 'D')
            ) as INV
            INNER JOIN
            (SELECT 
                DISTINCT
                MDS_FAM_ID as system_item_nbr,
                item_nbr,
                dept_nbr,
                subclass_nbr
            FROM
                {ITEM_DIM}
            WHERE dept_nbr in {CATS_SQL}) as ID
            ON INV.system_item_nbr = ID.system_item_nbr 
            )
    
    where snapshot_date = "{TODAY_DATE}")
    where inventory_qty >= 0
    and system_item_nbr is not null
    group by 1,2,3) as inv
    on item_mapping.child_item_nbr = inv.item_nbr
    and item_mapping.club_nbr = inv.club_nbr
    left join
    (select  
    item_nbr,
    store_nbr,
    sum(unit_qty) as unit_qty_1w
    from 
    
            (   SELECT
                s.store_nbr,
                s.scan_id,
                s.visit_date,
                s.unit_qty,
                s.retail_price,
                d.item_nbr
            FROM
                {SCAN} s,
                {STORE_INFO} si,
                (SELECT * FROM {VISIT} WHERE visit_date >= '{begin_iso}') v,
                (SELECT DISTINCT MDS_FAM_ID as scan_id, item_nbr FROM {ITEM_DIM} WHERE dept_nbr in (22,23,33,34,66,67,68,95,10,11,14,15,17,21,32,36,50,60,92,97,7,9,12,16,18,51,89,2,4,8,13,47,54,94,98,39,96,3,5,6,20,29,31,64,69,70,71,74,80,81,83,85,86,42,44,57,1, 40,48,52,58,78,41,43,46,49, 61,53,88)  AND
                 current_ind = 'Y' AND country_code = 'US' AND base_div_nbr = 18 AND subclass_nbr not in (61, 89, 91, 97)) d
            WHERE
                ------- JOINS -------
                s.store_nbr = si.store_nbr
                AND s.store_nbr = v.store_nbr
                AND s.visit_nbr = v.visit_nbr
                AND s.visit_date = v.visit_date
                AND s.scan_id = d.scan_id
                ------- FILTERS -------
                AND v.visit_subtype_code != 198
                AND s.SCAN_TYPE = 0
                and s.visit_date >='{begin_iso}'
                and s.visit_date < '{end_iso}'
                AND si.store_type NOT IN ('G', 'W')
                AND s.unit_qty > 0
                
            )
    
    where visit_date>="{SALES_MIN_DATE}" and visit_date<="{SALES_MAX_DATE}"
    and unit_qty > 0
    group by item_nbr ,store_nbr) as scan
    on item_mapping.child_item_nbr = scan.item_nbr
    and item_mapping.club_nbr = scan.store_nbr
    """
    
    return sql_mp_inv_scan
        
def raw_weights(df:PySparkDF)-> PySparkDF:
    parent_table = df.groupBy("parent_item_nbr", "club_nbr").agg(F.count("*").alias("num_child"), F.sum("inventory_qty").alias("inventory"), F.sum("unit_qty_1w").alias("units_1w"))
    df = df.join(parent_table, on=["parent_item_nbr", "club_nbr"], how="inner")
    df = df.withColumn("inv_weights", F.when(df.inventory == 0, 1/df.num_child).otherwise(df.inventory_qty/df.inventory))
    df = df.withColumn("sales_weights", F.when(df.units_1w == 0, 1/df.num_child).otherwise(df.unit_qty_1w/df.units_1w))   
    df = df.withColumn("weights", 0.7*df.inv_weights + 0.3*df.sales_weights)
    df = df.withColumn("inv_cap", F.when(df.weights == 0, 0).otherwise(df.inventory_qty/df.weights))
    return df
        
 
def scenarios(df: PySparkDF) -> PySparkDF:
    w1 = df.select("parent_item_nbr", "club_nbr", "inv_cap").distinct().alias("w1")
    w2 = df.select("parent_item_nbr", "club_nbr", "inv_cap").distinct()
    w2 = w2.withColumnRenamed("inv_cap", "inv_cap_limit")
    joined_df = w1.alias('w1').join(w2.alias('w2'), (w1.parent_item_nbr == w2.parent_item_nbr) & (w1.club_nbr == w2.club_nbr) & (w1.inv_cap < w2.inv_cap_limit))
    grouped_df = joined_df.groupBy("w1.parent_item_nbr", "w1.club_nbr", "w1.inv_cap").agg(min("w2.inv_cap_limit").alias("inv_max"))
    grouped_df = grouped_df.withColumnRenamed("inv_cap", "inv_min")
    window = Window.partitionBy(grouped_df.parent_item_nbr, grouped_df.club_nbr).orderBy(grouped_df.inv_min)
    result_df = grouped_df.select("*", row_number().over(window).alias("scenario"))
    return result_df


def disaggregation_weight_scenarios(df: PySparkDF, scenarios: PySparkDF) -> PySparkDF:
    df = df.join(scenarios, on=["parent_item_nbr", "club_nbr"], how="inner")
    df = df.withColumn("scenario_flag", when(col("inv_cap") >= col("inv_max"), 0).otherwise(1))
    df = df.select(df["parent_item_nbr"], df["child_item_nbr"], df["club_nbr"], df["num_child"],df["inventory_qty"],df["inventory"], df["inv_weights"], df["sales_weights"], df["weights"], df["inv_cap"],scenarios["inv_min"], scenarios["inv_max"], scenarios["scenario"], "scenario_flag")
    return df


def final_disaggregation(weights_df: PySparkDF) -> PySparkDF:
    total_weights = weights_df.withColumn('weights', when(col('scenario_flag') == 0, col('weights')).otherwise(0))
    total_weights = total_weights.groupBy(col('parent_item_nbr'), col('club_nbr'), col('scenario')).agg(sum('weights').alias('total_scenario_weights'))

    # Join the dataframes
    df = weights_df.join(total_weights, (weights_df.parent_item_nbr == total_weights.parent_item_nbr) & (weights_df.club_nbr == total_weights.club_nbr) & (weights_df.scenario == total_weights.scenario))

    # Add the final_weights column
    df = df.withColumn('final_weights', when(col('scenario_flag') == 0, col('weights') / col('total_scenario_weights')).otherwise(0))

    # Select the required columns
    df = df.select(weights_df["*"], 'total_scenario_weights', 'final_weights')
    return df

 
    
def generate_table_name(project_id, bq_dataset , table_name):
    return str(project_id+"."+bq_dataset+"."+table_name)
      
      
     
        
if __name__=='__main__':
    args = json.loads(sys.argv[1]) 
    project_id = args["GCP_PROJECT"] 
    bq_dataset = args["MATERIALIZATION_DATASET"] 
    ds_temp_bucket = args["GCS_TEMP_BUCKET"]
    source_dataset = args["FS_SOURCE_DATASET"]
    project_id_disaggregation = 'wmt-mlp-p-price-npd-pricing'

    spark = get_bq_spark_session(project_id, bq_dataset, ds_temp_bucket)
    spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")
    
    CUT_OFF_DATE_START =(datetime.datetime.today() - datetime.timedelta(days=30*45)).strftime("%Y-%m-%d")
    CUT_OFF_DATE_END=date.today().isoformat()
    CATS_SQL = (22,23,33,34,66,67,68,95,10,11,14,15,17,21,32,36,50,60,92,97,7,9,12,16,18,51,89,2,4,8,13,47,54,94,98,39,96,3,5,6,20,29,31,64,69,70,71,74,80,81,83,85,86,42,44,57,1, 40,48,52,58,78,41,43,46,49, 61,53,88)
    begin_iso, end_iso = get_date_range()
    # Input tables
    ITEM_DIM = "`wmt-edw-prod`.WW_CORE_DIM_VM.ITEM_DIM"
    MDSE_INV_DLY = "`prod-sams-cdp`.US_SAMS_PRODUCT360_CDP_VM.MDSE_INVENTORY_DLY"
    VISIT_MEMBER = '`wmt-edw-prod`.US_WC_MB_VM.VISIT_MEMBER'
    
    SCAN = "`wmt-edw-prod`.US_WC_MB_VM.SCANX"
    STORE_INFO = "`wmt-edw-prod`.US_WC_VM.STORE_INFO"
    VISIT = "`wmt-edw-prod` .US_WC_MB_VM.VISIT"
    
    SECOND_LEVEL_FEATURES=generate_table_name(project_id,source_dataset, "club_item_daily_v1")
    MAPPING_TABLE="`prod-sams-cdp`.US_SAMS_UNITY_CATALOG_CDP_GOLD_VM.OTM_ITEM_XREF"

    FINAL_WEIGHTS=generate_table_name(project_id_disaggregation, "markdown", "disaggregation_weights")
    TODAY_DATE = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")
    SALES_MAX_DATE = (datetime.datetime.today() - datetime.timedelta(days=2)).strftime("%Y-%m-%d")
    SALES_MIN_DATE = (datetime.datetime.today() - datetime.timedelta(days=8)).strftime("%Y-%m-%d")
    

    # merging second level features, mapping table and source tables
    sql_merge_map_inv_scan=merge_map_inv_scan_table()    
    map_inv_scan=read_bq_table_to_df(spark, sql_merge_map_inv_scan)
    raw_weights_df = raw_weights(map_inv_scan)
    scenarios_df = scenarios(raw_weights_df)
    disaggregation_weight_scenarios_df = disaggregation_weight_scenarios(raw_weights_df, scenarios_df)
    final_disaggregation_df = final_disaggregation(disaggregation_weight_scenarios_df)   

    write_df_to_bq_table( 
    df=final_disaggregation_df,
        write_mode='overwrite',
        bq_table=FINAL_WEIGHTS)
  
        
        
        
 
    
